import streamlit as st
import pandas as pd
import itertools
import re
import networkx as nx
import numpy as np
import warnings
from io import BytesIO

# openpyxl ê´€ë ¨ ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', category=UserWarning, module='openpyxl')

# --- í•¨ìˆ˜ ì •ì˜ ---

@st.cache_data
def create_yearly_edge_dfs(df):
    """
    ì…ë ¥ DataFrameì—ì„œ ì—°ë„ë³„ í˜‘ë ¥ ê´€ê³„(ì—£ì§€) DataFrame ë”•ì…”ë„ˆë¦¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # 'ì œNíŠ¹í—ˆê¶Œìëª…' í˜•íƒœì˜ ì»¬ëŸ¼ì„ ë™ì ìœ¼ë¡œ ì°¾ê¸°
    patent_holder_pattern = re.compile(r'ì œ(\d+)íŠ¹í—ˆê¶Œìëª…')
    dynamic_cols_to_combine = []
    for col in df.columns:
        match = patent_holder_pattern.match(col)
        if match:
            dynamic_cols_to_combine.append((int(match.group(1)), col))
    
    dynamic_cols_to_combine.sort()
    cols_to_combine = [col_name for _, col_name in dynamic_cols_to_combine]

    if not cols_to_combine:
        st.error("ì…ë ¥ íŒŒì¼ì— 'ì œNíŠ¹í—ˆê¶Œìëª…' í˜•íƒœì˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return None

    yearly_edge_dfs = {}
    try:
        if 'ë“±ë¡ë…„ë„' not in df.columns or 'í˜‘ë ¥ìœ í˜•4' not in df.columns:
            raise KeyError("'ë“±ë¡ë…„ë„' ë˜ëŠ” 'í˜‘ë ¥ìœ í˜•4' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        years_to_process = sorted(df['ë“±ë¡ë…„ë„'].dropna().unique().tolist())
        
        for year in years_to_process:
            df_filtered_by_year = df[df['ë“±ë¡ë…„ë„'] == year]
            df_final_filtered = df_filtered_by_year[df_filtered_by_year['í˜‘ë ¥ìœ í˜•4'] == 'ë²•ì¸ ê°„'].copy()
            
            if df_final_filtered.empty:
                continue

            all_combinations_for_this_year = []
            for _, row in df_final_filtered.iterrows():
                current_row_values_raw = [str(row[col]) for col in cols_to_combine if pd.notna(row[col])]
                current_row_unique_values = sorted(list(set(current_row_values_raw)))
                
                if len(current_row_unique_values) >= 2:
                    row_combinations = list(itertools.combinations(current_row_unique_values, 2))
                    all_combinations_for_this_year.extend(row_combinations)
            
            if not all_combinations_for_this_year:
                continue

            yearly_edge_dfs[str(int(year))] = pd.DataFrame(all_combinations_for_this_year, columns=['source', 'target'])
        
        return yearly_edge_dfs
    except KeyError as e:
        st.error(f"ì…ë ¥ íŒŒì¼ì— í•„ìˆ˜ ì»¬ëŸ¼({e})ì´ ì—†ìŠµë‹ˆë‹¤. ('ë“±ë¡ë…„ë„', 'í˜‘ë ¥ìœ í˜•4', 'ì œNíŠ¹í—ˆê¶Œìëª…')")
        return None

@st.cache_data
def analyze_networks(_edge_dfs):
    """
    ì—£ì§€ DataFrame ë”•ì…”ë„ˆë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì§€í‘œë¥¼ ê³„ì‚°í•˜ê³ ,
    ì „ì²´ ì§€í‘œ DataFrameê³¼ ì¤‘ì‹¬ì„± DataFrame ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    overall_metrics = []
    all_sheets_centrality_results = {}

    for year, df_sheet_data in _edge_dfs.items():
        G = nx.from_pandas_edgelist(df_sheet_data, 'source', 'target')

        if G.number_of_nodes() == 0:
            continue

        num_nodes = G.number_of_nodes()
        num_edges = G.number_of_edges()
        density = nx.density(G)
        average_degree = sum(d for _, d in G.degree()) / num_nodes if num_nodes > 0 else 0
        connected_components = list(nx.connected_components(G))
        num_components = len(connected_components)
        avg_path_length, num_nodes_giant_component, proportion_giant_component = np.nan, 0, 0.0
        if connected_components:
            largest_component_nodes = max(connected_components, key=len)
            subgraph_gc = G.subgraph(largest_component_nodes)
            if subgraph_gc.number_of_nodes() > 1:
                 avg_path_length = nx.average_shortest_path_length(subgraph_gc)
            num_nodes_giant_component = len(largest_component_nodes)
            if num_nodes > 0:
                proportion_giant_component = (num_nodes_giant_component / num_nodes) * 100
        max_eigenvalue, alpha_reciprocal = np.nan, np.nan
        if num_nodes > 1:
            try:
                adj_matrix = nx.to_numpy_array(G)
                eigenvalues = np.linalg.eigvals(adj_matrix)
                max_eigenvalue = np.max(eigenvalues.real)
                if max_eigenvalue > 1e-9:
                    alpha_reciprocal = 1.0 / max_eigenvalue
            except Exception: pass
        overall_metrics.append({
            'Year': year, 'Nodes': num_nodes, 'Edges': num_edges, 'Density': density,
            'Avg Degree': average_degree, 'Avg Path Length (GC)': avg_path_length,
            'Components': num_components, 'Giant Component Nodes': num_nodes_giant_component,
            'Giant Component Ratio (%)': proportion_giant_component, 'Max Eigenvalue': max_eigenvalue,
            'Alpha (1/Max Eigenvalue)': alpha_reciprocal
        })
        degree_centrality = nx.degree_centrality(G)
        closeness_centrality = nx.closeness_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        try:
            eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, tol=1.0e-8)
        except nx.PowerIterationFailedConvergence:
            eigenvector_centrality = {n: np.nan for n in G.nodes()}
            st.warning(f"'{year}'ë…„ ê³ ìœ ë²¡í„° ì¤‘ì‹¬ì„± ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ìˆ˜ë ´ ì‹¤íŒ¨).")
        katz_alpha = alpha_reciprocal * 0.9 if pd.notna(alpha_reciprocal) and alpha_reciprocal > 0 else 0.01
        try:
            katz_centrality = nx.katz_centrality(G, alpha=katz_alpha, max_iter=1000)
        except nx.PowerIterationFailedConvergence:
            katz_centrality = {n: np.nan for n in G.nodes()}
            st.warning(f"'{year}'ë…„ ê°€ì¸  ì¤‘ì‹¬ì„± ê³„ì‚°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ìˆ˜ë ´ ì‹¤íŒ¨).")
        df_centrality = pd.DataFrame({
            'ê¸°ê´€ëª…': list(G.nodes()),
            'ì—°ê²°ì¤‘ì‹¬ì„±': [degree_centrality.get(n, 0) for n in G.nodes()],
            'ê·¼ì ‘ì¤‘ì‹¬ì„±': [closeness_centrality.get(n, 0) for n in G.nodes()],
            'ë§¤ê°œì¤‘ì‹¬ì„±': [betweenness_centrality.get(n, 0) for n in G.nodes()],
            'ê³ ìœ ë²¡í„°ì¤‘ì‹¬ì„±': [eigenvector_centrality.get(n, 0) for n in G.nodes()],
            'ê°€ì¸ ì¤‘ì‹¬ì„±': [katz_centrality.get(n, 0) for n in G.nodes()],
        }).sort_values(by='ì—°ê²°ì¤‘ì‹¬ì„±', ascending=False)
        all_sheets_centrality_results[year] = df_centrality

    df_overall = pd.DataFrame(overall_metrics).set_index('Year')
    return df_overall, all_sheets_centrality_results

@st.cache_data
def to_excel(_df_dict):
    """
    DataFrame ë”•ì…”ë„ˆë¦¬ë¥¼ ì—‘ì…€ íŒŒì¼ í˜•ì‹ì˜ ë°”ì´íŠ¸ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        for sheet_name, df in _df_dict.items():
            df.to_excel(writer, sheet_name=sheet_name, index=False)
    processed_data = output.getvalue()
    return processed_data

def run():
    """
    SNA ì„œë¸Œì•±ì„ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    st.set_page_config(layout="wide")
    st.title("ğŸ”¬ ì‚¬íšŒì—°ê²°ë§ ë¶„ì„ (Social Network Analysis)")
    st.markdown("""
    ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ì—°ë„ë³„ í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    - **í•„ìˆ˜ ì»¬ëŸ¼**: `ë“±ë¡ë…„ë„`, `í˜‘ë ¥ìœ í˜•4`, `ì œ1íŠ¹í—ˆê¶Œìëª…`, `ì œ2íŠ¹í—ˆê¶Œìëª…`, ...
    - **ë¶„ì„ ì¡°ê±´**: `í˜‘ë ¥ìœ í˜•4`ê°€ 'ë²•ì¸ ê°„'ì¸ ë°ì´í„°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """)

    # --- 1. ë°ì´í„° ì…ë ¥ UI ---
    st.divider()
    st.header("1. ë°ì´í„° ì…ë ¥")
    uploaded_file = st.file_uploader("ë¶„ì„í•  ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=['xlsx'])

    if uploaded_file is not None:
        try:
            df_input = pd.read_excel(uploaded_file)
            st.success(f"âœ… '{uploaded_file.name}' íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            with st.expander("ì—…ë¡œë“œí•œ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê°œ í–‰)"):
                st.dataframe(df_input.head())

            # --- 2. ë™ì‘ ì‹¤í–‰ ë²„íŠ¼ ---
            st.divider()
            st.header("2. ë¶„ì„ ì‹¤í–‰")
            if st.button("ğŸš€ ë¶„ì„ ì‹¤í–‰í•˜ê¸°"):
                
                # --- 3. ê²°ê³¼ í‘œì‹œ ì˜ì—­ ---
                st.divider()
                st.header("3. ë¶„ì„ ê²°ê³¼")

                # ë‹¨ê³„ 1: ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
                with st.spinner("1ë‹¨ê³„: ì—°ë„ë³„ í˜‘ë ¥ ê´€ê³„(ì—£ì§€)ë¥¼ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                    edge_dfs = create_yearly_edge_dfs(df_input)
                
                if not edge_dfs:
                    st.error("ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í•˜ì—¬ ë¶„ì„ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. íŒŒì¼ì˜ ì»¬ëŸ¼ êµ¬ì„±ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    st.stop()
                st.success("âœ… 1ë‹¨ê³„: ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ!")

                # ë‹¨ê³„ 2: ë„¤íŠ¸ì›Œí¬ ë¶„ì„
                with st.spinner("2ë‹¨ê³„: ë„¤íŠ¸ì›Œí¬ ì§€í‘œ ë° ì¤‘ì‹¬ì„±ì„ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤..."):
                    df_metrics, df_centralities = analyze_networks(edge_dfs)

                if df_metrics is None or df_centralities is None:
                    st.error("ë„¤íŠ¸ì›Œí¬ ë¶„ì„ì— ì‹¤íŒ¨í•˜ì—¬ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    st.stop()
                st.success("âœ… 2ë‹¨ê³„: ë„¤íŠ¸ì›Œí¬ ë¶„ì„ ì™„ë£Œ!")

                # ê²°ê³¼ í‘œì‹œ
                st.subheader("ğŸ“Š ë„¤íŠ¸ì›Œí¬ ê±°ì‹œ ì§€í‘œ")
                st.markdown("ì—°ë„ë³„ ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œì…ë‹ˆë‹¤.")
                st.dataframe(df_metrics.style.format(precision=4))

                st.subheader("ğŸ”­ ë…¸ë“œ ì¤‘ì‹¬ì„± ë¶„ì„")
                st.markdown("ê° ì—°ë„ë³„ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì–´ë–¤ ê¸°ê´€ì´ ì¤‘ì‹¬ì ì¸ ì—­í• ì„ í•˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.")
                
                # ì¤‘ì‹¬ì„± ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                excel_data = to_excel(df_centralities)
                st.download_button(
                    label="ğŸ“¥ ì¤‘ì‹¬ì„± ë¶„ì„ ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (Excel)",
                    data=excel_data,
                    file_name="ì¤‘ì‹¬ì„±_ë¶„ì„_ê²°ê³¼.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

                years = list(df_centralities.keys())
                if not years:
                    st.warning("í‘œì‹œí•  ì¤‘ì‹¬ì„± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    sorted_years = sorted(years, key=int)
                    tabs = st.tabs(sorted_years)
                    for i, year in enumerate(sorted_years):
                        with tabs[i]:
                            st.dataframe(df_centralities[year])
        
        except Exception as e:
            st.error(f"íŒŒì¼ì„ ì½ê±°ë‚˜ ë¶„ì„í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    run()
